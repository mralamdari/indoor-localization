{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mralamdari/indoor-localization/blob/master/indoor_localization_Colab_Full_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZPQ66cNp5Sq"
      },
      "source": [
        "[Project Link](https://github.com/nikola310/indoor-localization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpC9MfnHqGp3"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/nikola310/indoor-localization.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHGDhwhCqr_u"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content/drive/MyDrive'\n",
        "!kaggle datasets download -d mehdimka/ble-rssi-dataset\n",
        "!unzip \\*.zip && rm *.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UeB0xtWnb_MN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from os.path import exists\n",
        "from os import mkdir\n",
        "\n",
        "min_max_ibeacons = False # change to True to see min/max values of the ibeacons\n",
        "print_location = False # change to True to see location values\n",
        "print_ibeacon_histograms = False # change to True to see ibeacon histograms\n",
        "save_class_histograms = False\n",
        "save_plots = True\n",
        "plots_directory = './plots/'\n",
        "\n",
        "def get_plots(dataframe):\n",
        "    if not exists(plots_directory):\n",
        "        mkdir(plots_directory)\n",
        "    dataframe.drop(['x', 'y'], axis=1, inplace=True)\n",
        "    column_names = dataframe.columns\n",
        "\n",
        "    for index, element in enumerate(dataframe.columns):\n",
        "        for _, next_element in enumerate(column_names[index+1:]):\n",
        "            plt.scatter(dataframe[element].values, dataframe[next_element])\n",
        "            filename = \"_\".join([element, next_element])\n",
        "\n",
        "            plt.savefig(\"\".join([plots_directory, filename]))\n",
        "            plt.clf()\n",
        "            plt.close()\n",
        "\n",
        "def ascii_to_coord(loc):\n",
        "    '''\n",
        "        Convert ASCII location to numerical coordinate\n",
        "\n",
        "        Parameters:\n",
        "            - loc = location as a letter\n",
        "\n",
        "        Returns:\n",
        "            - numerical coordinate scaled to A - W\n",
        "    '''\n",
        "    return ord(loc.upper()) - 64\n",
        "\n",
        "def extract_new_features_and_save_them(dataframe, file_name):\n",
        "    dataframe = dataframe.abs()\n",
        "\n",
        "    if 'x' in dataframe.columns:\n",
        "        column_names = list(dataframe.columns.values[:-2])\n",
        "    else:\n",
        "        column_names = list(dataframe.columns.values)\n",
        "\n",
        "    for index, element in enumerate(column_names):\n",
        "        for _, next_elem in enumerate(column_names[index+1:]):\n",
        "            dataframe[\"_\".join([element, next_elem])] = dataframe[element] - dataframe[next_elem]\n",
        "\n",
        "    dataframe.drop(column_names, axis=1, inplace=True)\n",
        "\n",
        "    if 'x' in dataframe.columns:\n",
        "        x_y = dataframe[['x', 'y']]\n",
        "\n",
        "        dataframe.drop(['x', 'y'], axis=1, inplace=True)\n",
        "        dataframe['x'] = x_y['x']\n",
        "        dataframe['y'] = x_y['y']\n",
        "\n",
        "    dataframe.to_csv(file_name, sep=',', index=False)\n",
        "\n",
        "def convert_location_to_x_y(dataframe):\n",
        "    df['x'] = df['location'].str[0]\n",
        "    df['y'] = df['location'].str[1:]\n",
        "\n",
        "    df['x'] = df['x'].apply(ascii_to_coord)\n",
        "    df['y'] = df['y'].astype(int)\n",
        "\n",
        "    df.drop(['location'], axis=1, inplace=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    df = pd.read_csv('iBeacon_RSSI_Labeled.csv')\n",
        "    dfU = pd.read_csv('iBeacon_RSSI_Unlabeled.csv')\n",
        "\n",
        "    # Location for unlabeled dataset is always the same, i.e. ?\n",
        "    # Date information is useless in this case.\n",
        "    dfU.drop(['location', 'date'], axis=1, inplace=True)\n",
        "    df.drop(['date'], axis=1, inplace=True)\n",
        "\n",
        "    if min_max_ibeacons:\n",
        "        print('Minimum values in each column of labeled data: ')\n",
        "        print(df.min())\n",
        "        print('Maximum values in each column of labeled data: ')\n",
        "        print(df.max())\n",
        "\n",
        "        print('Minimum values in each column of unlabeled data: ')\n",
        "        print(dfU.min())\n",
        "        print('Maximum values in each column of unlabeled data: ')\n",
        "        print(dfU.max())\n",
        "        # Values of ibeacons is always between -200 and -50\n",
        "\n",
        "    if print_location:\n",
        "        print(df.location)\n",
        "\n",
        "        print(df.location.max())\n",
        "        # Unlike in the image, values along the x axis go from A to W.\n",
        "\n",
        "    if save_class_histograms:\n",
        "        sns.countplot(x=\"location\", data=df)\n",
        "        location_occurences = df['location'].value_counts()\n",
        "        pd.set_option('display.max_rows', len(location_occurences))\n",
        "        location_dictionary = location_occurences.to_dict()\n",
        "        print(location_occurences)\n",
        "        print(sorted(location_dictionary))\n",
        "\n",
        "    convert_location_to_x_y(df)\n",
        "\n",
        "    if print_ibeacon_histograms:\n",
        "        for col in df.columns[0:10]:\n",
        "            df.hist(column = col)\n",
        "            plt.show()\n",
        "\n",
        "    if save_plots:\n",
        "        get_plots(df.copy(deep=True))\n",
        "\n",
        "\n",
        "    # Normalize data\n",
        "    float_labeled_data = df.iloc[:, :-2].values.astype(float)\n",
        "\n",
        "    min_max_scaler = MinMaxScaler()\n",
        "    scaled_labeled_data = min_max_scaler.fit_transform(float_labeled_data)\n",
        "\n",
        "    labeled_data_copy = df.copy(deep=True)\n",
        "\n",
        "    for i in range(13):\n",
        "        labeled_data_copy.iloc[:, i] = scaled_labeled_data[:, i]\n",
        "\n",
        "    float_unlabeled_data = dfU.values.astype(float)\n",
        "    scaled_unlabeled_data = min_max_scaler.fit_transform(float_unlabeled_data)\n",
        "\n",
        "    unlabeled_data_copy = dfU.copy(deep=True)\n",
        "\n",
        "    for i in range(13):\n",
        "        unlabeled_data_copy.iloc[:, i] = scaled_unlabeled_data[:, i]\n",
        "\n",
        "    # Save processed data\n",
        "    df.to_csv('data_labeled.csv', sep=',', index=False)\n",
        "    extract_new_features_and_save_them(df.copy(deep=True), 'extracted_features.csv')\n",
        "    labeled_data_copy.to_csv('labeled_data_scaled.csv', sep=',', index=False)\n",
        "    extract_new_features_and_save_them(labeled_data_copy.copy(deep=True), 'extracted_features_scaled.csv')\n",
        "    dfU.to_csv('data_unlabeled.csv', sep=',', index=False)\n",
        "    extract_new_features_and_save_them(dfU.copy(deep=True), 'extracted_features_unlabeled.csv')\n",
        "    unlabeled_data_copy.to_csv('unlabeled_data_scaled.csv', sep=',', index=False)\n",
        "    extract_new_features_and_save_them(unlabeled_data_copy.copy(deep=True), 'extracted_features_scaled_unlabeled.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "K3wCVMg5qBA3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from pandas import read_csv\n",
        "from pandas import DataFrame\n",
        "from tensorflow.keras import backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "        return K.sqrt(K.mean(K.square(y_pred - tf.cast(y_true, tf.float32)), axis=-1))\n",
        "\n",
        "def get_euclidean_distances_and_mean(points1, points2):\n",
        "    x1, y1 = points1\n",
        "    x2, y2 = points2\n",
        "    x1, y1 = np.array(x1), np.array(y1)\n",
        "    x2, y2 = np.array(x2), np.array(y2)\n",
        "    distance_x = x1 - x2\n",
        "    distance_y = y1 - y2\n",
        "    squared_distance_x = distance_x ** 2\n",
        "    squared_distance_y = distance_y ** 2\n",
        "    distances = squared_distance_x + squared_distance_y\n",
        "    euclidean_distances = np.sqrt(distances)\n",
        "    return euclidean_distances, np.mean(euclidean_distances)\n",
        "\n",
        "def create_neural_network(input_dimension):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(50, input_dim=input_dimension, activation='sigmoid'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dense(50, activation='relu'))\n",
        "    model.add(Dense(50, activation='relu'))\n",
        "    model.add(Dense(2, activation='relu'))\n",
        "\n",
        "    model.compile(loss='mse', optimizer=Adam(.001), metrics=['mse'])\n",
        "    return model\n",
        "\n",
        "def prepare_data_for_training(path_to_file, csv_file=True):\n",
        "    if csv_file:\n",
        "      data = read_csv(path_to_file, index_col=None)\n",
        "    else:\n",
        "      data = path_to_file\n",
        "    y = data.iloc[:, -2:]\n",
        "    x = data.iloc[:, :-2]\n",
        "\n",
        "    return train_test_split(x, y, test_size = .2, shuffle = False)\n",
        "\n",
        "def train_neural_network(neural_network, train_data, validation_data, number_of_epochs, batch_size):\n",
        "    es = EarlyStopping(monitor = 'val_loss', patience = 100, verbose = 0, mode = 'auto', restore_best_weights = True)\n",
        "    return neural_network.fit(x = train_data[0].astype(int), y = train_data[1].astype(int), validation_data = validation_data, epochs=number_of_epochs, batch_size=batch_size,  verbose=0, callbacks = [es])\n",
        "\n",
        "def plot_history(history):\n",
        "    if 'mse' in history.history.keys():\n",
        "        plt.plot(history.history['mse'])\n",
        "        plt.plot(history.history['val_mse'])\n",
        "        plt.title('Mean squared error')\n",
        "        plt.ylabel('Error')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend(['train', 'test'], loc='upper left')\n",
        "        plt.show()\n",
        "\n",
        "    if 'loss' in history.history.keys():\n",
        "        plt.plot(history.history['loss'])\n",
        "        plt.plot(history.history['val_loss'])\n",
        "        plt.title('Model loss')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend(['train', 'test'], loc='upper left')\n",
        "        plt.show()\n",
        "\n",
        "    if 'accuracy' in history.history.keys():\n",
        "        plt.plot(history.history['accuracy'])\n",
        "        plt.plot(history.history['val_accuracy'])\n",
        "        plt.title('Model accuracy')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend(['train', 'test'], loc='upper left')\n",
        "        plt.show()\n",
        "\n",
        "def plot_cumulative_density_function(distances, save_file_name):\n",
        "    sorted_distances = np.sort(distances)\n",
        "    prob_deep = 1. * np.arange(len(sorted_distances))/(len(sorted_distances) - 1)\n",
        "\n",
        "    _, axes = plt.subplots()\n",
        "    axes.plot(sorted_distances, prob_deep, color='black')\n",
        "    plt.title('CDF of Euclidean distance error')\n",
        "    plt.xlabel('Distance (m)')\n",
        "    plt.ylabel('Probability')\n",
        "    plt.grid(True)\n",
        "    gridlines = axes.get_xgridlines() + axes.get_ygridlines()\n",
        "\n",
        "    for line in gridlines:\n",
        "        line.set_linestyle('-.')\n",
        "\n",
        "    # 'Figure_CDF_error.png'\n",
        "    plt.savefig(save_file_name, dpi=300)\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "def run_script_on_data(filename, plot=True, csv_file=True):\n",
        "    train_x, validation_x, train_y, validation_y = prepare_data_for_training(filename, csv_file)\n",
        "\n",
        "    neural_network = create_neural_network(train_x.shape[1])\n",
        "\n",
        "    history = train_neural_network(neural_network, (train_x, train_y), (validation_x, validation_y), 1000, 1000)\n",
        "\n",
        "    predictions = neural_network.predict(validation_x)\n",
        "    _, mean_distance = get_euclidean_distances_and_mean((predictions[:, 0], predictions[:, 1]),\n",
        "                                                    (validation_y[\"x\"], validation_y[\"y\"]))\n",
        "\n",
        "    if plot:\n",
        "      print('Mean distance: ' + str(mean_distance))\n",
        "      plot_history(history)\n",
        "    return mean_distance\n",
        "\n",
        "def create_autoencoder(input_dimension):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(50, input_dim=input_dimension, activation='sigmoid'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dense(25, activation='relu'))\n",
        "    model.add(Dense(8, activation='relu'))\n",
        "    model.add(Dense(25, activation='relu'))\n",
        "    model.add(Dense(50, activation='relu'))\n",
        "    model.add(Dense(input_dimension, activation='sigmoid')) #13\n",
        "\n",
        "    model.compile(optimizer=Adam(.0001, clipnorm = .5, clipvalue = .5),\n",
        "              loss='mse', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def build_autoencoder_neural_network(autoencoder):\n",
        "    predictions = Dense(2, activation = 'relu')(autoencoder.layers[3].output)\n",
        "    neural_network = Model(inputs=autoencoder.input, outputs=predictions)\n",
        "    return neural_network\n",
        "\n",
        "def set_trainable_layers(model):\n",
        "    for layer in model.layers[:-2]:\n",
        "        layer.trainable = False\n",
        "\n",
        "def set_new_trainable_layers(model):\n",
        "    for layer in model.layers[:-2]:\n",
        "        layer.trainable = True\n",
        "\n",
        "\n",
        "def run_autoencoder_scenario_on_data(path_to_unlabeled_data, path_to_labeled_data):\n",
        "    unlabeled_data = read_csv(path_to_unlabeled_data, index_col=None)\n",
        "\n",
        "    train_unlabeled, validation_unlabeled = train_test_split(unlabeled_data, test_size = .2, shuffle = False)\n",
        "\n",
        "    autoencoder = create_autoencoder(train_unlabeled.shape[1])\n",
        "    history = train_neural_network(autoencoder, (train_unlabeled, train_unlabeled), (validation_unlabeled, validation_unlabeled), 1000, 1000)\n",
        "    plot_history(history)\n",
        "\n",
        "    neural_network = build_autoencoder_neural_network(autoencoder)\n",
        "    set_trainable_layers(neural_network)\n",
        "    neural_network.compile(optimizer=Adam(.001, clipnorm = .5, clipvalue = .5),\n",
        "              loss=rmse, metrics=['accuracy'])\n",
        "\n",
        "    train_x, validation_x, train_y, validation_y = prepare_data_for_training(path_to_labeled_data)\n",
        "\n",
        "    history = train_neural_network(neural_network, (train_x, train_y), (validation_x, validation_y), 1000, 50)\n",
        "\n",
        "    plot_history(history)\n",
        "    predictions = neural_network.predict(validation_x)\n",
        "    distances, mean_distance = get_euclidean_distances_and_mean((predictions[:, 0], predictions[:, 1]),\n",
        "                                                    (validation_y[\"x\"], validation_y[\"y\"]))\n",
        "\n",
        "    plot_cumulative_density_function(distances, 'CDF_train_new_decoder.png')\n",
        "    print('Mean distance: ' + str(mean_distance))\n",
        "\n",
        "    set_new_trainable_layers(neural_network)\n",
        "    neural_network.compile(optimizer=Adam(.001, clipnorm = .5, clipvalue = .5),\n",
        "              loss=rmse, metrics=['accuracy'])\n",
        "\n",
        "    train_x, validation_x, train_y, validation_y = prepare_data_for_training(path_to_labeled_data)\n",
        "\n",
        "    history = train_neural_network(neural_network, (train_x, train_y), (validation_x, validation_y), 1000, 50)\n",
        "\n",
        "    plot_history(history)\n",
        "\n",
        "    predictions = neural_network.predict(validation_x)\n",
        "    distances, mean_distance = get_euclidean_distances_and_mean((predictions[:, 0], predictions[:, 1]),\n",
        "                                                    (validation_y[\"x\"], validation_y[\"y\"]))\n",
        "\n",
        "    plot_cumulative_density_function(distances, 'CDF_train_full_neural_network.png')\n",
        "    print('Mean distance: ' + str(mean_distance))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_script_on_data('data_labeled.csv')\n",
        "    run_script_on_data('labeled_data_scaled.csv')\n",
        "\n",
        "    run_autoencoder_scenario_on_data('extracted_features_scaled_unlabeled.csv', 'extracted_features_scaled.csv')\n",
        "    run_autoencoder_scenario_on_data('unlabeled_data_scaled.csv', 'labeled_data_scaled.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O5dHOBjAkpPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tb4j_L-vv8Lr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1FaogNyxDyLYRDNoE4BY-UHSfqP8RnMQ7",
      "authorship_tag": "ABX9TyPfhqQ+AaNlAyVDVZSyQho6",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}